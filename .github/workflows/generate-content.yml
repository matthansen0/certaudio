name: Generate Content

on:
  workflow_dispatch:
    inputs:
      certificationId:
        description: 'Microsoft certification ID'
        required: true
        type: choice
        options:
          # === TEST MODE ===
          - test       # Single unit test (~$0.15) for development
          # === AZURE ===
          - az-900     # Azure Fundamentals
          - az-104     # Azure Administrator
          - az-204     # Azure Developer
          - az-305     # Azure Solutions Architect Expert
          - az-400     # Azure DevOps Engineer Expert
          - az-500     # Azure Security Engineer
          - az-700     # Azure Network Engineer
          - az-140     # Azure Virtual Desktop Specialty
          - az-800     # Windows Server Hybrid Administrator (exam 1)
          - az-801     # Windows Server Hybrid Administrator (exam 2)
          # === AGENTIC AI ===
          - ab-731     # AI Transformation Leader (beta)
          - ab-100     # Agentic AI Business Solutions Architect
          # === AI & DATA ===
          - ai-900     # Azure AI Fundamentals
          - ai-102     # Azure AI Engineer
          - dp-900     # Azure Data Fundamentals
          - dp-100     # Azure Data Scientist
          - dp-203     # Azure Data Engineer
          - dp-300     # Azure Database Administrator
          - dp-600     # Microsoft Fabric Analytics Engineer
          - dp-700     # Microsoft Fabric Data Engineer
          # === SECURITY ===
          - sc-900     # Security, Compliance, Identity Fundamentals
          - sc-100     # Cybersecurity Architect Expert
          - sc-200     # Security Operations Analyst
          - sc-300     # Identity and Access Administrator
          - sc-400     # Information Protection and Compliance Admin
          # === MICROSOFT 365 ===
          - ms-900     # Microsoft 365 Fundamentals
          - ms-102     # Microsoft 365 Administrator
          - ms-700     # Microsoft Teams Administrator
          - md-102     # Endpoint Administrator
          # === POWER PLATFORM ===
          - pl-900     # Power Platform Fundamentals
          - pl-100     # Power Platform App Maker
          - pl-200     # Power Platform Functional Consultant
          - pl-300     # Power BI Data Analyst
          - pl-400     # Power Platform Developer
          - pl-500     # Power Automate RPA Developer
          - pl-600     # Power Platform Solution Architect Expert
          # === DYNAMICS 365 ===
          - mb-910     # Dynamics 365 Fundamentals (CRM)
          - mb-920     # Dynamics 365 Fundamentals (ERP)
          - mb-210     # Dynamics 365 Sales Functional Consultant
          - mb-220     # Dynamics 365 Customer Insights - Journeys
          - mb-230     # Dynamics 365 Customer Service
          - mb-240     # Dynamics 365 Field Service
          - mb-260     # Dynamics 365 Customer Insights - Data
          - mb-300     # Dynamics 365 Core Finance and Operations
          - mb-310     # Dynamics 365 Finance Functional Consultant
          - mb-330     # Dynamics 365 Supply Chain Management
          - mb-335     # Dynamics 365 Supply Chain Management Expert
          - mb-500     # Dynamics 365 Finance & Operations Apps Developer
          - mb-700     # Dynamics 365 Finance & Operations Solution Architect
          - mb-800     # Dynamics 365 Business Central Functional Consultant
          - mb-820     # Dynamics 365 Business Central Developer
        default: 'test'
      audioFormat:
        description: 'Audio format'
        required: true
        type: choice
        options:
          - instructional
          - podcast
        default: 'instructional'
      instructionalVoice:
        description: 'Voice for instructional format (https://speech.microsoft.com/portal/voicegallery)'
        required: false
        type: choice
        options:
          # Dragon HD voices (highest quality)
          - en-US-Andrew:DragonHDLatestNeural   # Male, warm and professional (HD)
          - en-US-Ava:DragonHDLatestNeural      # Female, warm and engaging (HD)
          - en-US-Andrew2:DragonHDLatestNeural  # Male, conversational (HD)
          - en-US-Adam:DragonHDLatestNeural     # Male (HD)
          - en-US-Aria:DragonHDLatestNeural     # Female, newscast (HD)
          # Standard Neural voices
          - en-US-AndrewNeural      # Male, warm and professional
          - en-US-BrianNeural       # Male, conversational
          - en-US-GuyNeural         # Male, newscast style
          - en-US-DavisNeural       # Male, calm and clear
          - en-US-JasonNeural       # Male, friendly
          - en-US-TonyNeural        # Male, expressive
          - en-US-AvaNeural         # Female, warm and engaging
          - en-US-EmmaNeural        # Female, professional
          - en-US-JennyNeural       # Female, conversational
          - en-US-AriaNeural        # Female, newscast style
          - en-US-SaraNeural        # Female, friendly
        default: 'en-US-Andrew:DragonHDLatestNeural'
      podcastHostVoice:
        description: 'Host voice for podcast format'
        required: false
        type: choice
        options:
          # Dragon HD voices (highest quality)
          - en-US-Ava:DragonHDLatestNeural      # Female (HD, GA)
          - en-US-Emma:DragonHDLatestNeural     # Female (HD, GA)
          - en-US-Andrew:DragonHDLatestNeural   # Male (HD, GA)
          - en-US-Andrew2:DragonHDLatestNeural  # Male, conversational (HD, GA)
          - en-US-Adam:DragonHDLatestNeural     # Male (HD, GA)
          # Preview HD voices (eastus/westeurope/southeastasia only)
          - en-US-Ava3:DragonHDLatestNeural     # Female, podcast-optimized (HD, Preview)
          - en-US-Andrew3:DragonHDLatestNeural  # Male, podcast-optimized (HD, Preview)
          # Standard Neural voices
          - en-US-AvaNeural
          - en-US-AndrewNeural
          - en-US-BrianNeural
          - en-US-GuyNeural
          - en-US-DavisNeural
          - en-US-JasonNeural
          - en-US-TonyNeural
          - en-US-EmmaNeural
          - en-US-JennyNeural
          - en-US-AriaNeural
        default: 'en-US-Ava:DragonHDLatestNeural'
      podcastExpertVoice:
        description: 'Expert voice for podcast format'
        required: false
        type: choice
        options:
          # Dragon HD voices (highest quality)
          - en-US-Andrew:DragonHDLatestNeural   # Male (HD, GA)
          - en-US-Andrew2:DragonHDLatestNeural  # Male, conversational (HD, GA)
          - en-US-Adam:DragonHDLatestNeural     # Male (HD, GA)
          - en-US-Ava:DragonHDLatestNeural      # Female (HD, GA)
          - en-US-Emma:DragonHDLatestNeural     # Female (HD, GA)
          # Preview HD voices (eastus/westeurope/southeastasia only)
          - en-US-Andrew3:DragonHDLatestNeural  # Male, podcast-optimized (HD, Preview)
          - en-US-Ava3:DragonHDLatestNeural     # Female, podcast-optimized (HD, Preview)
          - en-US-Aria:DragonHDLatestNeural     # Female (HD, Preview)
          # Standard Neural voices
          - en-US-AndrewNeural
          - en-US-BrianNeural
          - en-US-GuyNeural
          - en-US-DavisNeural
          - en-US-JasonNeural
          - en-US-TonyNeural
          - en-US-AvaNeural
          - en-US-EmmaNeural
          - en-US-JennyNeural
          - en-US-AriaNeural
        default: 'en-US-Andrew:DragonHDLatestNeural'
      forceRegenerate:
        description: 'Regenerate all episodes (purges existing episodes first, use for major changes)'
        required: false
        type: boolean
        default: false

permissions:
  id-token: write
  contents: read

env:
  AZURE_SUBSCRIPTION_ID: ${{ secrets.AZURE_SUBSCRIPTION_ID }}
  AZURE_RESOURCE_GROUP: ${{ secrets.AZURE_RESOURCE_GROUP }}
  EPISODE_BATCH_SIZE: 10
  # Use pinned suffix or fallback to run_id for resource naming
  UNIQUE_SUFFIX: ${{ secrets.AZURE_UNIQUE_SUFFIX || github.run_id }}
  # Location for ephemeral AI Search (East US has better capacity than Central US)
  LOCATION: 'eastus'
  # Fixed search name pattern (avoids GitHub secret masking issues with outputs)
  SEARCH_NAME: 'certaudio-search-ephemeral'

# =============================================================================
# REQUIRED SECRETS (configure in GitHub Settings > Secrets > Actions):
# - AZURE_CLIENT_ID:        Service principal/app registration client ID
# - AZURE_TENANT_ID:        Azure AD tenant ID  
# - AZURE_SUBSCRIPTION_ID:  Azure subscription ID
# - AZURE_RESOURCE_GROUP:   Resource group name (e.g., rg-certaudio-dev)
# =============================================================================

jobs:
  check-secrets:
    name: Validate Required Secrets
    runs-on: ubuntu-latest
    steps:
      - name: Check required secrets are configured
        run: |
          missing=""
          if [ -z "${{ secrets.AZURE_CLIENT_ID }}" ]; then missing="$missing AZURE_CLIENT_ID"; fi
          if [ -z "${{ secrets.AZURE_TENANT_ID }}" ]; then missing="$missing AZURE_TENANT_ID"; fi
          if [ -z "${{ secrets.AZURE_SUBSCRIPTION_ID }}" ]; then missing="$missing AZURE_SUBSCRIPTION_ID"; fi
          if [ -z "${{ secrets.AZURE_RESOURCE_GROUP }}" ]; then missing="$missing AZURE_RESOURCE_GROUP"; fi
          if [ -n "$missing" ]; then
            echo "::error::Missing required secrets:$missing"
            echo ""
            echo "Configure these secrets in GitHub: Settings > Secrets and variables > Actions"
            echo "See README.md for setup instructions."
            exit 1
          fi
          echo "‚úÖ All required secrets are configured"

  # =========================================================================
  # EPHEMERAL AI SEARCH - Deploy on-demand, delete after generation
  # Saves ~$250/month by not keeping Search running when not generating content
  # =========================================================================
  deploy-search:
    name: Deploy Ephemeral AI Search
    needs: check-secrets
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Azure Login
        uses: azure/login@v2
        with:
          client-id: ${{ secrets.AZURE_CLIENT_ID }}
          tenant-id: ${{ secrets.AZURE_TENANT_ID }}
          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}

      - name: Get automation principal ID
        id: principal
        run: |
          accessToken=$(az account get-access-token \
            --resource https://management.azure.com \
            --query accessToken -o tsv)
          
          principalId=$(python3 - <<'PY'
          import os, json, base64
          token = os.environ.get('ACCESS_TOKEN', '')
          parts = token.split('.')
          if len(parts) < 2:
            print('')
            raise SystemExit(0)
          payload = parts[1]
          payload += '=' * (-len(payload) % 4)
          data = base64.urlsafe_b64decode(payload.encode('utf-8')).decode('utf-8')
          claims = json.loads(data)
          print(claims.get('oid') or claims.get('objectId') or '')
          PY
          )
          echo "principalId=$principalId" >> $GITHUB_OUTPUT
        env:
          ACCESS_TOKEN: ${{ steps.principal.outputs.accessToken }}

      - name: Deploy AI Search
        id: deploy
        uses: azure/cli@v2
        with:
          inlineScript: |
            set -euo pipefail
            
            # Check for and clean up any stuck/failed Search service
            # This prevents "conflict" errors when a previous deployment left a zombie resource
            echo "Checking for existing Search service..."
            existingStatus=$(az rest --method get \
              --url "https://management.azure.com/subscriptions/$AZURE_SUBSCRIPTION_ID/resourceGroups/$AZURE_RESOURCE_GROUP/providers/Microsoft.Search/searchServices/$SEARCH_NAME?api-version=2023-11-01" \
              --query "properties.provisioningState" -o tsv 2>/dev/null || echo "NotFound")
            
            if [[ "$existingStatus" == "failed" ]] || [[ "$existingStatus" == "Failed" ]]; then
              echo "‚ö†Ô∏è  Found Search service in failed state - deleting before redeploy..."
              az search service delete -g "$AZURE_RESOURCE_GROUP" -n "$SEARCH_NAME" --yes || true
              sleep 10
            elif [[ "$existingStatus" != "NotFound" ]] && [[ "$existingStatus" != "succeeded" ]] && [[ "$existingStatus" != "Succeeded" ]]; then
              echo "‚ö†Ô∏è  Found Search service in '$existingStatus' state - deleting before redeploy..."
              az search service delete -g "$AZURE_RESOURCE_GROUP" -n "$SEARCH_NAME" --yes || true
              sleep 10
            elif [[ "$existingStatus" == "succeeded" ]] || [[ "$existingStatus" == "Succeeded" ]]; then
              echo "‚úÖ Search service already exists and is healthy - skipping deployment"
              echo "   Endpoint: https://${SEARCH_NAME}.search.windows.net"
              exit 0
            fi
            
            # Get principal ID for RBAC
            accessToken=$(az account get-access-token \
              --resource https://management.azure.com \
              --query accessToken -o tsv)
            
            principalId=$(python3 - "$accessToken" <<'PY'
            import sys, json, base64
            token = sys.argv[1] if len(sys.argv) > 1 else ''
            parts = token.split('.')
            if len(parts) < 2:
              print('')
              raise SystemExit(0)
            payload = parts[1]
            payload += '=' * (-len(payload) % 4)
            data = base64.urlsafe_b64decode(payload.encode('utf-8')).decode('utf-8')
            claims = json.loads(data)
            print(claims.get('oid') or claims.get('objectId') or '')
            PY
            )
            
            echo "Deploying ephemeral AI Search with principal ID: $principalId"
            echo "Using fixed search name: $SEARCH_NAME"
            
            deploymentName="search-ephemeral-${{ github.run_id }}"
            
            # Deploy with fixed search name to avoid output masking issues
            az deployment group create \
              --name "$deploymentName" \
              --resource-group "$AZURE_RESOURCE_GROUP" \
              --template-file ./infra/modules/search-ephemeral.bicep \
              --parameters \
                searchName="$SEARCH_NAME" \
                location="$LOCATION" \
                automationPrincipalId="$principalId" \
              --only-show-errors
            
            echo "‚úÖ AI Search deployed: $SEARCH_NAME"
            echo "   Endpoint: https://${SEARCH_NAME}.search.windows.net"
            
            # Wait for RBAC to propagate
            echo "Waiting 30s for RBAC propagation..."
            sleep 30

  discover-content:
    name: Discover Exam Content
    needs: [check-secrets, deploy-search]
    runs-on: ubuntu-latest
    outputs:
      skillsOutline: ${{ steps.discover.outputs.skillsOutline }}
      sourceUrls: ${{ steps.discover.outputs.sourceUrls }}
      batchIndices: ${{ steps.batches.outputs.batchIndices }}
      mainSkillCount: ${{ steps.batches.outputs.mainSkillCount }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: src/pipeline/requirements.txt

      - name: Install dependencies
        run: |
          cd src/pipeline
          pip install -r requirements.txt

      - name: Azure Login
        uses: azure/login@v2
        with:
          client-id: ${{ secrets.AZURE_CLIENT_ID }}
          tenant-id: ${{ secrets.AZURE_TENANT_ID }}
          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}

      - name: Resolve endpoints for latest deployment
        run: |
          set -euo pipefail
          chmod +x ./scripts/get-endpoints.sh

          suffix="${{ secrets.AZURE_UNIQUE_SUFFIX }}"
          if [[ -n "$suffix" ]]; then
            out=$(./scripts/get-endpoints.sh "$AZURE_RESOURCE_GROUP" "$suffix")
          else
            out=$(./scripts/get-endpoints.sh "$AZURE_RESOURCE_GROUP")
          fi

          echo "$out" | awk -F= '/^(OPENAI_ENDPOINT|SPEECH_ENDPOINT|SPEECH_REGION|DOCUMENT_INTELLIGENCE_ENDPOINT|SEARCH_ENDPOINT|COSMOS_DB_ENDPOINT|STORAGE_ACCOUNT_NAME)=/ {print $0}' >> "$GITHUB_ENV"

      - name: Ensure Cosmos SQL RBAC for this workflow principal
        run: |
          set -euo pipefail

          accessToken=$(az account get-access-token \
            --resource https://management.azure.com \
            --query accessToken -o tsv)
          export ACCESS_TOKEN="$accessToken"

          principalObjectId=$(python3 - <<'PY'
          import os, json, base64
          token = os.environ.get('ACCESS_TOKEN', '')
          parts = token.split('.')
          if len(parts) < 2:
            print('')
            raise SystemExit(0)
          payload = parts[1]
          payload += '=' * (-len(payload) % 4)
          data = base64.urlsafe_b64decode(payload.encode('utf-8')).decode('utf-8')
          claims = json.loads(data)
          print(claims.get('oid') or claims.get('objectId') or '')
          PY
          )

          if [[ -z "$principalObjectId" ]]; then
            echo "::warning::Could not determine automation principal objectId; skipping Cosmos SQL RBAC check."
            exit 0
          fi

          cosmosName=$(echo "$COSMOS_DB_ENDPOINT" | sed -E 's#^https?://([^./]+)\..*$#\1#')
          if [[ -z "$cosmosName" ]]; then
            echo "::error::Could not parse Cosmos account name from COSMOS_DB_ENDPOINT=$COSMOS_DB_ENDPOINT"
            exit 1
          fi

          existing=$(az cosmosdb sql role assignment list \
            --account-name "$cosmosName" \
            --resource-group "$AZURE_RESOURCE_GROUP" \
            --query "[?principalId=='$principalObjectId' && scope=='/dbs/certaudio'] | length(@)" \
            -o tsv || echo "0")

          if [[ "$existing" == "0" ]]; then
            az cosmosdb sql role assignment create \
              --account-name "$cosmosName" \
              --resource-group "$AZURE_RESOURCE_GROUP" \
              --principal-id "$principalObjectId" \
              --scope "/dbs/certaudio" \
              --role-definition-id "00000000-0000-0000-0000-000000000002" \
              --only-show-errors
          fi

      - name: Discover exam content
        id: discover
        env:
          CERTIFICATION_ID: ${{ inputs.certificationId }}
        run: |
          cd src/pipeline
          
          # Handle test mode: use deep_discover --test for minimal content
          if [ "$CERTIFICATION_ID" = "test" ]; then
            echo "Running TEST MODE discovery (single unit, ~\$0.15)"
            python3 -m tools.deep_discover --test --output-file /tmp/discovery_results.json
          else
            # Combined strategy (learning paths + exam skills)
            echo "Running COMBINED discovery (learning paths + exam skills)"
            python3 -m tools.deep_discover \
              --certification-id "$CERTIFICATION_ID" \
              --comprehensive \
              --output-file /tmp/discovery_results.json
          fi
          
          echo "skillsOutline=$(cat /tmp/discovery_results.json | jq -c '.skillsOutline')" >> $GITHUB_OUTPUT
          echo "sourceUrls=$(cat /tmp/discovery_results.json | jq -c '.sourceUrls')" >> $GITHUB_OUTPUT

      - name: Purge existing episodes (if forceRegenerate)
        if: ${{ inputs.forceRegenerate == true }}
        env:
          CERTIFICATION_ID: ${{ inputs.certificationId }}
          AUDIO_FORMAT: ${{ inputs.audioFormat }}
        run: |
          echo "Force regenerate enabled - purging existing episodes for $CERTIFICATION_ID/$AUDIO_FORMAT"
          
          cd src/pipeline
          python3 - <<'PY'
          import os
          from azure.cosmos import CosmosClient
          from azure.cosmos.exceptions import CosmosResourceNotFoundError
          from azure.identity import DefaultAzureCredential
          from azure.storage.blob import BlobServiceClient
          from azure.core.exceptions import ResourceNotFoundError
          
          cert_id = os.environ["CERTIFICATION_ID"]
          audio_format = os.environ["AUDIO_FORMAT"]
          cosmos_endpoint = os.environ["COSMOS_DB_ENDPOINT"]
          storage_account = os.environ["STORAGE_ACCOUNT_NAME"]
          
          credential = DefaultAzureCredential()
          
          # Purge from Cosmos DB (if container exists)
          print("Purging episodes from Cosmos DB...")
          try:
              cosmos = CosmosClient(cosmos_endpoint, credential)
              db = cosmos.get_database_client("certaudio")
              container = db.get_container_client("episodes")
              
              query = f"SELECT c.id FROM c WHERE c.certificationId = '{cert_id}' AND c.audioFormat = '{audio_format}'"
              items = list(container.query_items(query, enable_cross_partition_query=True))
              
              for item in items:
                  try:
                      container.delete_item(item["id"], partition_key=cert_id)
                      print(f"  Deleted: {item['id']}")
                  except Exception as e:
                      print(f"  Warning: Could not delete {item['id']}: {e}")
              
              print(f"Deleted {len(items)} episode documents from Cosmos DB")
          except CosmosResourceNotFoundError:
              print("  Cosmos container 'episodes' does not exist yet - nothing to purge")
          except Exception as e:
              print(f"  Warning: Could not access Cosmos DB: {e}")
          
          # Purge from Blob Storage (if container exists)
          print("Purging audio/scripts from Blob Storage...")
          try:
              blob_url = f"https://{storage_account}.blob.core.windows.net"
              blob_service = BlobServiceClient(blob_url, credential)
              container_client = blob_service.get_container_client("content")
              
              # Check if container exists first
              if not container_client.exists():
                  print("  Blob container 'content' does not exist yet - nothing to purge")
              else:
                  prefixes = [
                      f"{cert_id}/{audio_format}/episodes/",
                      f"{cert_id}/{audio_format}/scripts/",
                      f"{cert_id}/{audio_format}/ssml/"
                  ]
                  
                  deleted_blobs = 0
                  for prefix in prefixes:
                      blobs = container_client.list_blobs(name_starts_with=prefix)
                      for blob in blobs:
                          try:
                              container_client.delete_blob(blob.name)
                              deleted_blobs += 1
                          except Exception as e:
                              print(f"  Warning: Could not delete {blob.name}: {e}")
                  
                  print(f"Deleted {deleted_blobs} blobs from storage")
          except ResourceNotFoundError:
              print("  Blob container 'content' does not exist yet - nothing to purge")
          except Exception as e:
              print(f"  Warning: Could not access Blob Storage: {e}")
          
          print("Purge complete - ready for fresh generation")
          PY

      - name: Compute episode batch matrix
        id: batches
        env:
          SKILLS_OUTLINE: ${{ steps.discover.outputs.skillsOutline }}
          BATCH_SIZE: ${{ env.EPISODE_BATCH_SIZE }}
          TOPICS_PER_EPISODE: 5
        run: |
          python3 - <<'PY'
          import json, math, os
          skills_outline = os.environ.get('SKILLS_OUTLINE', '[]')
          batch_size = int(os.environ.get('BATCH_SIZE', '10'))
          topics_per_ep = int(os.environ.get('TOPICS_PER_EPISODE', '5'))
          skills = json.loads(skills_outline)
          main_skills = [s for s in skills if s.get('topics') and len(s['topics']) > 0]
          
          if len(main_skills) == 0:
            raise SystemExit('No main skills discovered (skills with topics). Refusing to continue.')
          
          # Calculate total episode units (groups of ~5 topics)
          episode_units = 0
          for skill in main_skills:
            n_topics = len(skill.get('topics', []))
            units_for_skill = max(1, math.ceil(n_topics / topics_per_ep))
            episode_units += units_for_skill
          
          batches = max(1, math.ceil(episode_units / batch_size))
          indices = list(range(batches))
          out_path = os.environ['GITHUB_OUTPUT']
          with open(out_path, 'a', encoding='utf-8') as f:
            # mainSkillCount now represents total episode units for proper validation
            f.write(f"mainSkillCount={episode_units}\n")
            f.write(f"batchIndices={json.dumps(indices)}\n")
          print(f"Domains: {len(main_skills)}; Episode units (~{topics_per_ep} topics each): {episode_units}")
          print(f"Batch size: {batch_size}; Batches needed: {batches}; Indices: {indices}")
          PY

  index-content:
    name: Index Content for RAG
    runs-on: ubuntu-latest
    needs: [discover-content, deploy-search]
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: src/pipeline/requirements.txt

      - name: Install dependencies
        run: |
          cd src/pipeline
          pip install -r requirements.txt

      - name: Azure Login
        uses: azure/login@v2
        with:
          client-id: ${{ secrets.AZURE_CLIENT_ID }}
          tenant-id: ${{ secrets.AZURE_TENANT_ID }}
          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}

      - name: Resolve endpoints for latest deployment
        run: |
          set -euo pipefail
          chmod +x ./scripts/get-endpoints.sh

          suffix="${{ secrets.AZURE_UNIQUE_SUFFIX }}"
          if [[ -n "$suffix" ]]; then
            out=$(./scripts/get-endpoints.sh "$AZURE_RESOURCE_GROUP" "$suffix")
          else
            out=$(./scripts/get-endpoints.sh "$AZURE_RESOURCE_GROUP")
          fi

          # Get endpoints from always-on infra (excludes Search which is ephemeral)
          echo "$out" | awk -F= '/^(OPENAI_ENDPOINT|SPEECH_ENDPOINT|SPEECH_REGION|DOCUMENT_INTELLIGENCE_ENDPOINT|COSMOS_DB_ENDPOINT|STORAGE_ACCOUNT_NAME)=/ {print $0}' >> "$GITHUB_ENV"
          
          # Construct Search endpoint from fixed name (avoids GitHub secret masking)
          echo "SEARCH_ENDPOINT=https://${SEARCH_NAME}.search.windows.net" >> "$GITHUB_ENV"

      - name: Ensure Cosmos, Speech, OpenAI, and Storage RBAC for this workflow principal
        run: |
          set -euo pipefail

          accessToken=$(az account get-access-token \
            --resource https://management.azure.com \
            --query accessToken -o tsv)
          export ACCESS_TOKEN="$accessToken"

          principalObjectId=$(python3 - <<'PY'
          import os, json, base64
          token = os.environ.get('ACCESS_TOKEN', '')
          parts = token.split('.')
          if len(parts) < 2:
            print('')
            raise SystemExit(0)
          payload = parts[1]
          payload += '=' * (-len(payload) % 4)
          data = base64.urlsafe_b64decode(payload.encode('utf-8')).decode('utf-8')
          claims = json.loads(data)
          print(claims.get('oid') or claims.get('objectId') or '')
          PY
          )

          if [[ -z "$principalObjectId" ]]; then
            echo "::warning::Could not determine automation principal objectId; skipping RBAC checks."
            exit 0
          fi
          echo "Principal objectId: $principalObjectId"

          # --- Cosmos SQL RBAC ---
          cosmosName=$(echo "$COSMOS_DB_ENDPOINT" | sed -E 's#^https?://([^./]+)\..*$#\1#')
          if [[ -z "$cosmosName" ]]; then
            echo "::error::Could not parse Cosmos account name from COSMOS_DB_ENDPOINT=$COSMOS_DB_ENDPOINT"
            exit 1
          fi

          existing=$(az cosmosdb sql role assignment list \
            --account-name "$cosmosName" \
            --resource-group "$AZURE_RESOURCE_GROUP" \
            --query "[?principalId=='$principalObjectId' && scope=='/dbs/certaudio'] | length(@)" \
            -o tsv || echo "0")

          if [[ "$existing" == "0" ]]; then
            echo "Creating Cosmos SQL role assignment..."
            az cosmosdb sql role assignment create \
              --account-name "$cosmosName" \
              --resource-group "$AZURE_RESOURCE_GROUP" \
              --principal-id "$principalObjectId" \
              --scope "/dbs/certaudio" \
              --role-definition-id "00000000-0000-0000-0000-000000000002" \
              --only-show-errors
          else
            echo "Cosmos SQL RBAC already exists."
          fi

          # NOTE: Azure AI Search RBAC is handled in deploy-search job via Bicep

          # --- Azure OpenAI RBAC ---
          openAiName=$(echo "$OPENAI_ENDPOINT" | sed -E 's#^https?://([^./]+)\..*$#\1#')
          if [[ -z "$openAiName" ]]; then
            echo "::error::Could not parse OpenAI account name from OPENAI_ENDPOINT=$OPENAI_ENDPOINT"
            exit 1
          fi

          openAiId=$(az resource show \
            --resource-group "$AZURE_RESOURCE_GROUP" \
            --name "$openAiName" \
            --resource-type "Microsoft.CognitiveServices/accounts" \
            --query id -o tsv)

          if [[ -z "$openAiId" ]]; then
            echo "::error::Could not resolve OpenAI resource ID for $openAiName"
            exit 1
          fi

          existing=$(az role assignment list \
            --assignee "$principalObjectId" \
            --scope "$openAiId" \
            --query "[?roleDefinitionName=='Cognitive Services OpenAI User'] | length(@)" \
            -o tsv || echo "0")

          if [[ "$existing" == "0" ]]; then
            echo "Creating Cognitive Services OpenAI User role assignment..."
            az role assignment create \
              --assignee-object-id "$principalObjectId" \
              --assignee-principal-type ServicePrincipal \
              --role "Cognitive Services OpenAI User" \
              --scope "$openAiId" \
              --only-show-errors

            # Role assignments can take a short time to propagate.
            echo "Waiting 30s for RBAC propagation..."
            sleep 30
          else
            echo "OpenAI RBAC already exists."
          fi

          # --- Azure Storage Blob RBAC ---
          storageId=$(az resource show \
            --resource-group "$AZURE_RESOURCE_GROUP" \
            --name "$STORAGE_ACCOUNT_NAME" \
            --resource-type "Microsoft.Storage/storageAccounts" \
            --query id -o tsv)

          if [[ -z "$storageId" ]]; then
            echo "::error::Could not resolve Storage resource ID for $STORAGE_ACCOUNT_NAME"
            exit 1
          fi

          existing=$(az role assignment list \
            --assignee "$principalObjectId" \
            --scope "$storageId" \
            --query "[?roleDefinitionName=='Storage Blob Data Contributor'] | length(@)" \
            -o tsv || echo "0")

          if [[ "$existing" == "0" ]]; then
            echo "Creating Storage Blob Data Contributor role assignment..."
            az role assignment create \
              --assignee-object-id "$principalObjectId" \
              --assignee-principal-type ServicePrincipal \
              --role "Storage Blob Data Contributor" \
              --scope "$storageId" \
              --only-show-errors

            # Role assignments can take a short time to propagate.
            echo "Waiting 30s for RBAC propagation..."
            sleep 30
          else
            echo "Storage RBAC already exists."
          fi

          # --- Azure Speech RBAC ---
          speechName=$(echo "$SPEECH_ENDPOINT" | sed -E 's#^https?://([^./]+)\..*$#\1#')
          if [[ -z "$speechName" ]]; then
            echo "::error::Could not parse Speech account name from SPEECH_ENDPOINT=$SPEECH_ENDPOINT"
            exit 1
          fi

          speechId=$(az resource show \
            --resource-group "$AZURE_RESOURCE_GROUP" \
            --name "$speechName" \
            --resource-type "Microsoft.CognitiveServices/accounts" \
            --query id -o tsv)

          if [[ -z "$speechId" ]]; then
            echo "::error::Could not resolve Speech resource ID for $speechName"
            exit 1
          fi

          existing=$(az role assignment list \
            --assignee "$principalObjectId" \
            --scope "$speechId" \
            --query "[?roleDefinitionName=='Cognitive Services User'] | length(@)" \
            -o tsv || echo "0")

          if [[ "$existing" == "0" ]]; then
            echo "Creating Cognitive Services User role assignment on Speech resource..."
            az role assignment create \
              --assignee-object-id "$principalObjectId" \
              --assignee-principal-type ServicePrincipal \
              --role "Cognitive Services User" \
              --scope "$speechId" \
              --only-show-errors

            echo "Waiting 30s for RBAC propagation..."
            sleep 30
          else
            echo "Speech RBAC already exists."
          fi

      - name: Index content in AI Search
        env:
          SOURCE_URLS: ${{ needs.discover-content.outputs.sourceUrls }}
          CERTIFICATION_ID: ${{ inputs.certificationId }}
        run: |
          cd src/pipeline
          python3 -m tools.index_content \
            --certification-id "$CERTIFICATION_ID" \
            --source-urls "$SOURCE_URLS"

  generate-episodes:
    name: Generate Episodes
    runs-on: ubuntu-latest
    needs: [discover-content, deploy-search, index-content]
    strategy:
      fail-fast: true  # Stop all batches on first failure to save compute
      matrix:
        batch: ${{ fromJson(needs.discover-content.outputs.batchIndices) }}
      max-parallel: 1  # Sequential batches - TTS is now parallelized within each batch
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: src/pipeline/requirements.txt

      - name: Install dependencies
        run: |
          cd src/pipeline
          pip install -r requirements.txt

      - name: Azure Login
        uses: azure/login@v2
        with:
          client-id: ${{ secrets.AZURE_CLIENT_ID }}
          tenant-id: ${{ secrets.AZURE_TENANT_ID }}
          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}

      - name: Resolve endpoints for latest deployment
        run: |
          set -euo pipefail
          chmod +x ./scripts/get-endpoints.sh

          suffix="${{ secrets.AZURE_UNIQUE_SUFFIX }}"
          if [[ -n "$suffix" ]]; then
            out=$(./scripts/get-endpoints.sh "$AZURE_RESOURCE_GROUP" "$suffix")
          else
            out=$(./scripts/get-endpoints.sh "$AZURE_RESOURCE_GROUP")
          fi

          # Get endpoints from always-on infra (excludes Search which is ephemeral)
          echo "$out" | awk -F= '/^(OPENAI_ENDPOINT|SPEECH_ENDPOINT|SPEECH_REGION|DOCUMENT_INTELLIGENCE_ENDPOINT|COSMOS_DB_ENDPOINT|STORAGE_ACCOUNT_NAME)=/ {print $0}' >> "$GITHUB_ENV"
          
          # Construct Search endpoint from fixed name (avoids GitHub secret masking)
          echo "SEARCH_ENDPOINT=https://${SEARCH_NAME}.search.windows.net" >> "$GITHUB_ENV"

      - name: Generate episode batch
        env:
          CERTIFICATION_ID: ${{ inputs.certificationId }}
          AUDIO_FORMAT: ${{ inputs.audioFormat }}
          INSTRUCTIONAL_VOICE: ${{ inputs.instructionalVoice }}
          PODCAST_HOST_VOICE: ${{ inputs.podcastHostVoice }}
          PODCAST_EXPERT_VOICE: ${{ inputs.podcastExpertVoice }}
          FORCE_REGENERATE: ${{ inputs.forceRegenerate }}
          SKILLS_OUTLINE: ${{ needs.discover-content.outputs.skillsOutline }}
          BATCH_INDEX: ${{ matrix.batch }}
          # Pass OIDC credentials for periodic re-authentication
          AZURE_CLIENT_ID: ${{ secrets.AZURE_CLIENT_ID }}
          AZURE_TENANT_ID: ${{ secrets.AZURE_TENANT_ID }}
          AZURE_SUBSCRIPTION_ID: ${{ secrets.AZURE_SUBSCRIPTION_ID }}
        run: |
          # Start background token refresh to prevent OIDC token expiration.
          # GitHub Actions OIDC tokens cannot be refreshed - they expire after ~5 minutes.
          # We must periodically re-authenticate using az login with federated credentials.
          # The ACTIONS_ID_TOKEN_REQUEST_* vars are set by azure/login and enable OIDC re-auth.
          (
            while true; do
              sleep 240  # Re-authenticate every 4 minutes (before 5-min expiry)
              echo "[$(date -Iseconds)] Refreshing OIDC authentication..."
              az login --service-principal \
                -u "$AZURE_CLIENT_ID" \
                -t "$AZURE_TENANT_ID" \
                --federated-token "$(curl -sLS "${ACTIONS_ID_TOKEN_REQUEST_URL}&audience=api://AzureADTokenExchange" -H "Authorization: bearer ${ACTIONS_ID_TOKEN_REQUEST_TOKEN}" | jq -r '.value')" \
                > /dev/null 2>&1 || echo "[$(date -Iseconds)] Warning: Token refresh failed"
              az account set --subscription "$AZURE_SUBSCRIPTION_ID" > /dev/null 2>&1 || true
            done
          ) &
          TOKEN_REFRESH_PID=$!
          trap "kill $TOKEN_REFRESH_PID 2>/dev/null || true" EXIT
          
          cd src/pipeline

          # Combined discovery tends to produce broad coverage; keep a stable minimum.
          export MIN_WORDS_PER_PART=1200

          # TTS chunking thresholds (audio-only). These do NOT affect narration/transcript length.
          # Keep single-request synthesis under ~10 minutes; 1600 words is a practical upper bound.
          export TTS_SINGLE_REQUEST_MAX_WORDS=1600
          export TTS_MAX_WORDS_PER_SEGMENT=1400
          
          # Build command with optional --force-regenerate flag
          REGEN_FLAG=""
          if [ "$FORCE_REGENERATE" = "true" ]; then
            REGEN_FLAG="--force-regenerate"
          fi
          
          python3 -m tools.generate_episodes \
            --certification-id "$CERTIFICATION_ID" \
            --audio-format "$AUDIO_FORMAT" \
            --instructional-voice "$INSTRUCTIONAL_VOICE" \
            --podcast-host-voice "$PODCAST_HOST_VOICE" \
            --podcast-expert-voice "$PODCAST_EXPERT_VOICE" \
            --skills-outline "$SKILLS_OUTLINE" \
            --batch-index "$BATCH_INDEX" \
            --batch-size "$EPISODE_BATCH_SIZE" \
            $REGEN_FLAG

  finalize:
    name: Finalize Content Generation
    runs-on: ubuntu-latest
    needs: [discover-content, generate-episodes]
    # Run finalize even if some batches failed - we want to keep successfully generated episodes
    if: always() && needs.discover-content.result == 'success'
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: src/pipeline/requirements.txt

      - name: Azure Login
        uses: azure/login@v2
        with:
          client-id: ${{ secrets.AZURE_CLIENT_ID }}
          tenant-id: ${{ secrets.AZURE_TENANT_ID }}
          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}

      - name: Resolve endpoints for latest deployment
        run: |
          set -euo pipefail
          chmod +x ./scripts/get-endpoints.sh

          suffix="${{ secrets.AZURE_UNIQUE_SUFFIX }}"
          if [[ -n "$suffix" ]]; then
            out=$(./scripts/get-endpoints.sh "$AZURE_RESOURCE_GROUP" "$suffix")
          else
            out=$(./scripts/get-endpoints.sh "$AZURE_RESOURCE_GROUP")
          fi

          echo "$out" | awk -F= '/^(OPENAI_ENDPOINT|SPEECH_ENDPOINT|SPEECH_REGION|DOCUMENT_INTELLIGENCE_ENDPOINT|SEARCH_ENDPOINT|COSMOS_DB_ENDPOINT|STORAGE_ACCOUNT_NAME)=/ {print $0}' >> "$GITHUB_ENV"

      - name: Ensure Cosmos and Storage RBAC for this workflow principal
        run: |
          set -euo pipefail

          accessToken=$(az account get-access-token \
            --resource https://management.azure.com \
            --query accessToken -o tsv)
          export ACCESS_TOKEN="$accessToken"

          principalObjectId=$(python3 - <<'PY'
          import os, json, base64
          token = os.environ.get('ACCESS_TOKEN', '')
          parts = token.split('.')
          if len(parts) < 2:
            print('')
            raise SystemExit(0)
          payload = parts[1]
          payload += '=' * (-len(payload) % 4)
          data = base64.urlsafe_b64decode(payload.encode('utf-8')).decode('utf-8')
          claims = json.loads(data)
          print(claims.get('oid') or claims.get('objectId') or '')
          PY
          )

          if [[ -z "$principalObjectId" ]]; then
            echo "::warning::Could not determine automation principal objectId; skipping Cosmos SQL RBAC check."
            exit 0
          fi

          cosmosName=$(echo "$COSMOS_DB_ENDPOINT" | sed -E 's#^https?://([^./]+)\..*$#\1#')
          if [[ -z "$cosmosName" ]]; then
            echo "::error::Could not parse Cosmos account name from COSMOS_DB_ENDPOINT=$COSMOS_DB_ENDPOINT"
            exit 1
          fi

          existing=$(az cosmosdb sql role assignment list \
            --account-name "$cosmosName" \
            --resource-group "$AZURE_RESOURCE_GROUP" \
            --query "[?principalId=='$principalObjectId' && scope=='/dbs/certaudio'] | length(@)" \
            -o tsv || echo "0")

          if [[ "$existing" == "0" ]]; then
            az cosmosdb sql role assignment create \
              --account-name "$cosmosName" \
              --resource-group "$AZURE_RESOURCE_GROUP" \
              --principal-id "$principalObjectId" \
              --scope "/dbs/certaudio" \
              --role-definition-id "00000000-0000-0000-0000-000000000002" \
              --only-show-errors
          fi

          # --- Azure Storage Blob RBAC ---
          storageId=$(az resource show \
            --resource-group "$AZURE_RESOURCE_GROUP" \
            --name "$STORAGE_ACCOUNT_NAME" \
            --resource-type "Microsoft.Storage/storageAccounts" \
            --query id -o tsv)

          if [[ -z "$storageId" ]]; then
            echo "::error::Could not resolve Storage resource ID for $STORAGE_ACCOUNT_NAME"
            exit 1
          fi

          existing=$(az role assignment list \
            --assignee "$principalObjectId" \
            --scope "$storageId" \
            --query "[?roleDefinitionName=='Storage Blob Data Contributor'] | length(@)" \
            -o tsv || echo "0")

          if [[ "$existing" == "0" ]]; then
            echo "Creating Storage Blob Data Contributor role assignment..."
            az role assignment create \
              --assignee-object-id "$principalObjectId" \
              --assignee-principal-type ServicePrincipal \
              --role "Storage Blob Data Contributor" \
              --scope "$storageId" \
              --only-show-errors

            echo "Waiting 30s for RBAC propagation..."
            sleep 30
          else
            echo "Storage RBAC already exists."
          fi

      - name: Generate episode index
        env:
          CERTIFICATION_ID: ${{ inputs.certificationId }}
          AUDIO_FORMAT: ${{ inputs.audioFormat }}
        run: |
          cd src/pipeline
          pip install -r requirements.txt
          python3 -m tools.generate_index \
            --certification-id "$CERTIFICATION_ID" \
            --audio-format "$AUDIO_FORMAT" \
            --min-episodes "${{ needs.discover-content.outputs.mainSkillCount }}"

      - name: Output generation summary
        run: |
          echo "## Content Generation Complete! :headphones:" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Setting | Value |" >> $GITHUB_STEP_SUMMARY
          echo "|---------|-------|" >> $GITHUB_STEP_SUMMARY
          echo "| Certification | ${{ inputs.certificationId }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Format | ${{ inputs.audioFormat }} |" >> $GITHUB_STEP_SUMMARY

  # =========================================================================
  # CLEANUP - Delete ephemeral AI Search to save ~$250/month
  # Always runs, even if generation failed, to avoid leaving resources behind
  # =========================================================================
  cleanup-search:
    name: Delete Ephemeral AI Search
    needs: [deploy-search, generate-episodes, finalize]
    if: always()
    runs-on: ubuntu-latest
    steps:
      - name: Azure Login
        uses: azure/login@v2
        with:
          client-id: ${{ secrets.AZURE_CLIENT_ID }}
          tenant-id: ${{ secrets.AZURE_TENANT_ID }}
          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}

      - name: Delete AI Search service
        uses: azure/cli@v2
        with:
          inlineScript: |
            set -euo pipefail
            
            echo "Deleting ephemeral AI Search: $SEARCH_NAME"
            
            # Check if it exists first
            exists=$(az search service show \
              --name "$SEARCH_NAME" \
              --resource-group "$AZURE_RESOURCE_GROUP" \
              --query name -o tsv 2>/dev/null || echo "")
            
            if [[ -n "$exists" ]]; then
              az search service delete \
                --name "$SEARCH_NAME" \
                --resource-group "$AZURE_RESOURCE_GROUP" \
                --yes
              
              echo "‚úÖ AI Search deleted: $SEARCH_NAME"
              echo "   Monthly savings: ~\$250"
            else
              echo "‚ö†Ô∏è AI Search not found (may have been deleted already): $SEARCH_NAME"
            fi
      
      - name: Summary
        run: |
          echo "## üßπ Cleanup Complete" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Ephemeral AI Search service deleted to save ~\$250/month." >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "The following always-on resources remain:" >> $GITHUB_STEP_SUMMARY
          echo "- Storage Account (audio files)" >> $GITHUB_STEP_SUMMARY
          echo "- Cosmos DB (episode metadata)" >> $GITHUB_STEP_SUMMARY
          echo "- Azure Functions (API)" >> $GITHUB_STEP_SUMMARY
          echo "- Static Web App (website)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Estimated monthly cost: ~\$5-10" >> $GITHUB_STEP_SUMMARY
