name: Generate Content

on:
  workflow_dispatch:
    inputs:
      certificationId:
        description: 'Microsoft certification ID'
        required: true
        type: choice
        options:
          # === TEST MODE ===
          - test       # Single unit test (~$0.15) for development
          # === AZURE ===
          - az-900     # Azure Fundamentals
          - az-104     # Azure Administrator
          - az-204     # Azure Developer
          - az-305     # Azure Solutions Architect Expert
          - az-400     # Azure DevOps Engineer Expert
          - az-500     # Azure Security Engineer
          - az-700     # Azure Network Engineer
          - az-140     # Azure Virtual Desktop Specialty
          - az-800     # Windows Server Hybrid Administrator (exam 1)
          - az-801     # Windows Server Hybrid Administrator (exam 2)
          # === AI & DATA ===
          - ai-900     # Azure AI Fundamentals
          - ai-102     # Azure AI Engineer
          - dp-900     # Azure Data Fundamentals
          - dp-100     # Azure Data Scientist
          - dp-203     # Azure Data Engineer
          - dp-300     # Azure Database Administrator
          - dp-600     # Microsoft Fabric Analytics Engineer
          - dp-700     # Microsoft Fabric Data Engineer (if available)
          # === SECURITY ===
          - sc-900     # Security, Compliance, Identity Fundamentals
          - sc-100     # Cybersecurity Architect Expert
          - sc-200     # Security Operations Analyst
          - sc-300     # Identity and Access Administrator
          - sc-400     # Information Protection and Compliance Admin
          # === MICROSOFT 365 ===
          - ms-900     # Microsoft 365 Fundamentals
          - ms-102     # Microsoft 365 Administrator
          - ms-700     # Microsoft Teams Administrator
          - md-102     # Endpoint Administrator
          # === POWER PLATFORM ===
          - pl-900     # Power Platform Fundamentals
          - pl-100     # Power Platform App Maker
          - pl-200     # Power Platform Functional Consultant
          - pl-300     # Power BI Data Analyst
          - pl-400     # Power Platform Developer
          - pl-500     # Power Automate RPA Developer
          - pl-600     # Power Platform Solution Architect Expert
          # === DYNAMICS 365 ===
          - mb-910     # Dynamics 365 Fundamentals (CRM)
          - mb-920     # Dynamics 365 Fundamentals (ERP)
          - mb-210     # Dynamics 365 Sales Functional Consultant
          - mb-220     # Dynamics 365 Customer Insights - Journeys
          - mb-230     # Dynamics 365 Customer Service
          - mb-240     # Dynamics 365 Field Service
          - mb-260     # Dynamics 365 Customer Insights - Data
          - mb-300     # Dynamics 365 Core Finance and Operations
          - mb-310     # Dynamics 365 Finance Functional Consultant
          - mb-330     # Dynamics 365 Supply Chain Management
          - mb-335     # Dynamics 365 Supply Chain Management Expert
          - mb-500     # Dynamics 365 Finance & Operations Apps Developer
          - mb-700     # Dynamics 365 Finance & Operations Solution Architect
          - mb-800     # Dynamics 365 Business Central Functional Consultant
          - mb-820     # Dynamics 365 Business Central Developer
        default: 'test'
      discoveryMode:
        description: 'Content discovery mode'
        required: true
        type: choice
        options:
          - deep       # Full learning paths (comprehensive, higher cost)
          - skills     # Skills outline only (quick, lower cost)
        default: 'deep'
      audioFormat:
        description: 'Audio format'
        required: true
        type: choice
        options:
          - instructional
          - podcast
        default: 'instructional'
      instructionalVoice:
        description: 'Voice for instructional format (https://speech.microsoft.com/portal/voicegallery)'
        required: false
        type: choice
        options:
          - en-US-AndrewNeural      # Male, warm and professional
          - en-US-BrianNeural       # Male, conversational
          - en-US-GuyNeural         # Male, newscast style
          - en-US-DavisNeural       # Male, calm and clear
          - en-US-JasonNeural       # Male, friendly
          - en-US-TonyNeural        # Male, expressive
          - en-US-AvaNeural         # Female, warm and engaging
          - en-US-EmmaNeural        # Female, professional
          - en-US-JennyNeural       # Female, conversational
          - en-US-AriaNeural        # Female, newscast style
          - en-US-SaraNeural        # Female, friendly
        default: 'en-US-AndrewNeural'
      podcastHostVoice:
        description: 'Host voice for podcast format'
        required: false
        type: choice
        options:
          - en-US-AndrewNeural
          - en-US-BrianNeural
          - en-US-GuyNeural
          - en-US-DavisNeural
          - en-US-JasonNeural
          - en-US-TonyNeural
          - en-US-AvaNeural
          - en-US-EmmaNeural
          - en-US-JennyNeural
          - en-US-AriaNeural
        default: 'en-US-GuyNeural'
      podcastExpertVoice:
        description: 'Expert voice for podcast format'
        required: false
        type: choice
        options:
          - en-US-AndrewNeural
          - en-US-BrianNeural
          - en-US-GuyNeural
          - en-US-DavisNeural
          - en-US-JasonNeural
          - en-US-TonyNeural
          - en-US-AvaNeural
          - en-US-EmmaNeural
          - en-US-JennyNeural
          - en-US-AriaNeural
        default: 'en-US-TonyNeural'
      forceRegenerate:
        description: 'Regenerate all episodes even if they already exist (use for voice changes)'
        required: false
        type: boolean
        default: false
      examPageUrl:
        description: 'Override exam page URL (optional, auto-discovered if empty)'
        required: false
        default: ''

permissions:
  id-token: write
  contents: read

env:
  AZURE_SUBSCRIPTION_ID: ${{ secrets.AZURE_SUBSCRIPTION_ID }}
  AZURE_RESOURCE_GROUP: ${{ secrets.AZURE_RESOURCE_GROUP }}
  EPISODE_BATCH_SIZE: 10
  # Use pinned suffix or fallback to run_id for resource naming
  UNIQUE_SUFFIX: ${{ secrets.AZURE_UNIQUE_SUFFIX || github.run_id }}
  # Location for ephemeral AI Search
  LOCATION: 'centralus'

# =============================================================================
# REQUIRED SECRETS (configure in GitHub Settings > Secrets > Actions):
# - AZURE_CLIENT_ID:        Service principal/app registration client ID
# - AZURE_TENANT_ID:        Azure AD tenant ID  
# - AZURE_SUBSCRIPTION_ID:  Azure subscription ID
# - AZURE_RESOURCE_GROUP:   Resource group name (e.g., rg-certaudio-dev)
# =============================================================================

jobs:
  check-secrets:
    name: Validate Required Secrets
    runs-on: ubuntu-latest
    steps:
      - name: Check required secrets are configured
        run: |
          missing=""
          if [ -z "${{ secrets.AZURE_CLIENT_ID }}" ]; then missing="$missing AZURE_CLIENT_ID"; fi
          if [ -z "${{ secrets.AZURE_TENANT_ID }}" ]; then missing="$missing AZURE_TENANT_ID"; fi
          if [ -z "${{ secrets.AZURE_SUBSCRIPTION_ID }}" ]; then missing="$missing AZURE_SUBSCRIPTION_ID"; fi
          if [ -z "${{ secrets.AZURE_RESOURCE_GROUP }}" ]; then missing="$missing AZURE_RESOURCE_GROUP"; fi
          if [ -n "$missing" ]; then
            echo "::error::Missing required secrets:$missing"
            echo ""
            echo "Configure these secrets in GitHub: Settings > Secrets and variables > Actions"
            echo "See README.md for setup instructions."
            exit 1
          fi
          echo "âœ… All required secrets are configured"

  # =========================================================================
  # EPHEMERAL AI SEARCH - Deploy on-demand, delete after generation
  # Saves ~$250/month by not keeping Search running when not generating content
  # =========================================================================
  deploy-search:
    name: Deploy Ephemeral AI Search
    needs: check-secrets
    runs-on: ubuntu-latest
    outputs:
      searchEndpoint: ${{ steps.deploy.outputs.searchEndpoint }}
      searchName: ${{ steps.deploy.outputs.searchName }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Azure Login
        uses: azure/login@v2
        with:
          client-id: ${{ secrets.AZURE_CLIENT_ID }}
          tenant-id: ${{ secrets.AZURE_TENANT_ID }}
          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}

      - name: Get automation principal ID
        id: principal
        run: |
          accessToken=$(az account get-access-token \
            --resource https://management.azure.com \
            --query accessToken -o tsv)
          
          principalId=$(python3 - <<'PY'
          import os, json, base64
          token = os.environ.get('ACCESS_TOKEN', '')
          parts = token.split('.')
          if len(parts) < 2:
            print('')
            raise SystemExit(0)
          payload = parts[1]
          payload += '=' * (-len(payload) % 4)
          data = base64.urlsafe_b64decode(payload.encode('utf-8')).decode('utf-8')
          claims = json.loads(data)
          print(claims.get('oid') or claims.get('objectId') or '')
          PY
          )
          echo "principalId=$principalId" >> $GITHUB_OUTPUT
        env:
          ACCESS_TOKEN: ${{ steps.principal.outputs.accessToken }}

      - name: Deploy AI Search
        id: deploy
        uses: azure/cli@v2
        with:
          inlineScript: |
            set -euo pipefail
            
            # Get principal ID for RBAC
            accessToken=$(az account get-access-token \
              --resource https://management.azure.com \
              --query accessToken -o tsv)
            
            principalId=$(python3 - "$accessToken" <<'PY'
            import sys, json, base64
            token = sys.argv[1] if len(sys.argv) > 1 else ''
            parts = token.split('.')
            if len(parts) < 2:
              print('')
              raise SystemExit(0)
            payload = parts[1]
            payload += '=' * (-len(payload) % 4)
            data = base64.urlsafe_b64decode(payload.encode('utf-8')).decode('utf-8')
            claims = json.loads(data)
            print(claims.get('oid') or claims.get('objectId') or '')
            PY
            )
            
            echo "Deploying ephemeral AI Search with principal ID: $principalId"
            
            deploymentName="search-ephemeral-${{ github.run_id }}"
            
            result=$(az deployment group create \
              --name "$deploymentName" \
              --resource-group "$AZURE_RESOURCE_GROUP" \
              --template-file ./infra/modules/search-ephemeral.bicep \
              --parameters \
                resourcePrefix="certaudio-dev" \
                location="$LOCATION" \
                uniqueSuffix="$UNIQUE_SUFFIX" \
                automationPrincipalId="$principalId" \
              --query 'properties.outputs' \
              -o json)
            
            searchEndpoint=$(echo "$result" | jq -r '.searchEndpoint.value')
            searchName=$(echo "$result" | jq -r '.searchName.value')
            
            echo "searchEndpoint=$searchEndpoint" >> $GITHUB_OUTPUT
            echo "searchName=$searchName" >> $GITHUB_OUTPUT
            
            echo "âœ… AI Search deployed: $searchName"
            echo "   Endpoint: $searchEndpoint"
            
            # Wait for RBAC to propagate
            echo "Waiting 30s for RBAC propagation..."
            sleep 30

  discover-content:
    name: Discover Exam Content
    needs: [check-secrets, deploy-search]
    runs-on: ubuntu-latest
    outputs:
      skillsOutline: ${{ steps.discover.outputs.skillsOutline }}
      sourceUrls: ${{ steps.discover.outputs.sourceUrls }}
      batchIndices: ${{ steps.batches.outputs.batchIndices }}
      mainSkillCount: ${{ steps.batches.outputs.mainSkillCount }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: src/pipeline/requirements.txt

      - name: Install dependencies
        run: |
          cd src/pipeline
          pip install -r requirements.txt

      - name: Azure Login
        uses: azure/login@v2
        with:
          client-id: ${{ secrets.AZURE_CLIENT_ID }}
          tenant-id: ${{ secrets.AZURE_TENANT_ID }}
          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}

      - name: Resolve endpoints for latest deployment
        run: |
          set -euo pipefail
          chmod +x ./scripts/get-endpoints.sh

          suffix="${{ secrets.AZURE_UNIQUE_SUFFIX }}"
          if [[ -n "$suffix" ]]; then
            out=$(./scripts/get-endpoints.sh "$AZURE_RESOURCE_GROUP" "$suffix")
          else
            out=$(./scripts/get-endpoints.sh "$AZURE_RESOURCE_GROUP")
          fi

          echo "$out" | awk -F= '/^(OPENAI_ENDPOINT|SPEECH_ENDPOINT|SPEECH_REGION|DOCUMENT_INTELLIGENCE_ENDPOINT|SEARCH_ENDPOINT|COSMOS_DB_ENDPOINT|STORAGE_ACCOUNT_NAME)=/ {print $0}' >> "$GITHUB_ENV"

      - name: Ensure Cosmos SQL RBAC for this workflow principal
        run: |
          set -euo pipefail

          accessToken=$(az account get-access-token \
            --resource https://management.azure.com \
            --query accessToken -o tsv)
          export ACCESS_TOKEN="$accessToken"

          principalObjectId=$(python3 - <<'PY'
          import os, json, base64
          token = os.environ.get('ACCESS_TOKEN', '')
          parts = token.split('.')
          if len(parts) < 2:
            print('')
            raise SystemExit(0)
          payload = parts[1]
          payload += '=' * (-len(payload) % 4)
          data = base64.urlsafe_b64decode(payload.encode('utf-8')).decode('utf-8')
          claims = json.loads(data)
          print(claims.get('oid') or claims.get('objectId') or '')
          PY
          )

          if [[ -z "$principalObjectId" ]]; then
            echo "::warning::Could not determine automation principal objectId; skipping Cosmos SQL RBAC check."
            exit 0
          fi

          cosmosName=$(echo "$COSMOS_DB_ENDPOINT" | sed -E 's#^https?://([^./]+)\..*$#\1#')
          if [[ -z "$cosmosName" ]]; then
            echo "::error::Could not parse Cosmos account name from COSMOS_DB_ENDPOINT=$COSMOS_DB_ENDPOINT"
            exit 1
          fi

          existing=$(az cosmosdb sql role assignment list \
            --account-name "$cosmosName" \
            --resource-group "$AZURE_RESOURCE_GROUP" \
            --query "[?principalId=='$principalObjectId' && scope=='/dbs/certaudio'] | length(@)" \
            -o tsv || echo "0")

          if [[ "$existing" == "0" ]]; then
            az cosmosdb sql role assignment create \
              --account-name "$cosmosName" \
              --resource-group "$AZURE_RESOURCE_GROUP" \
              --principal-id "$principalObjectId" \
              --scope "/dbs/certaudio" \
              --role-definition-id "00000000-0000-0000-0000-000000000002" \
              --only-show-errors
          fi

      - name: Discover exam content
        id: discover
        env:
          CERTIFICATION_ID: ${{ inputs.certificationId }}
          EXAM_PAGE_URL: ${{ inputs.examPageUrl }}
          DISCOVERY_MODE: ${{ inputs.discoveryMode }}
        run: |
          cd src/pipeline
          
          # Handle test mode: use deep_discover --test for minimal content
          if [ "$CERTIFICATION_ID" = "test" ]; then
            echo "Running TEST MODE discovery (single unit, ~\$0.15)"
            python3 -m tools.deep_discover --test --output-file /tmp/discovery_results.json
          elif [ "$DISCOVERY_MODE" = "deep" ]; then
            echo "Running DEEP discovery (full learning paths)"
            python3 -m tools.deep_discover \
              --certification-id "$CERTIFICATION_ID" \
              --output-file /tmp/discovery_results.json
          else
            echo "Running SKILLS OUTLINE discovery (quick mode)"
            python3 -m tools.discover_exam_content \
              --certification-id "$CERTIFICATION_ID" \
              --exam-page-url "$EXAM_PAGE_URL" \
              --output-file /tmp/discovery_results.json
          fi
          
          echo "skillsOutline=$(cat /tmp/discovery_results.json | jq -c '.skillsOutline')" >> $GITHUB_OUTPUT
          echo "sourceUrls=$(cat /tmp/discovery_results.json | jq -c '.sourceUrls')" >> $GITHUB_OUTPUT

      - name: Compute episode batch matrix
        id: batches
        env:
          SKILLS_OUTLINE: ${{ steps.discover.outputs.skillsOutline }}
          BATCH_SIZE: ${{ env.EPISODE_BATCH_SIZE }}
          TOPICS_PER_EPISODE: 5
        run: |
          python3 - <<'PY'
          import json, math, os
          skills_outline = os.environ.get('SKILLS_OUTLINE', '[]')
          batch_size = int(os.environ.get('BATCH_SIZE', '10'))
          topics_per_ep = int(os.environ.get('TOPICS_PER_EPISODE', '5'))
          skills = json.loads(skills_outline)
          main_skills = [s for s in skills if s.get('topics') and len(s['topics']) > 0]
          
          if len(main_skills) == 0:
            raise SystemExit('No main skills discovered (skills with topics). Refusing to continue.')
          
          # Calculate total episode units (groups of ~5 topics)
          episode_units = 0
          for skill in main_skills:
            n_topics = len(skill.get('topics', []))
            units_for_skill = max(1, math.ceil(n_topics / topics_per_ep))
            episode_units += units_for_skill
          
          batches = max(1, math.ceil(episode_units / batch_size))
          indices = list(range(batches))
          out_path = os.environ['GITHUB_OUTPUT']
          with open(out_path, 'a', encoding='utf-8') as f:
            # mainSkillCount now represents total episode units for proper validation
            f.write(f"mainSkillCount={episode_units}\n")
            f.write(f"batchIndices={json.dumps(indices)}\n")
          print(f"Domains: {len(main_skills)}; Episode units (~{topics_per_ep} topics each): {episode_units}")
          print(f"Batch size: {batch_size}; Batches needed: {batches}; Indices: {indices}")
          PY

  index-content:
    name: Index Content for RAG
    runs-on: ubuntu-latest
    needs: [discover-content, deploy-search]
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: src/pipeline/requirements.txt

      - name: Install dependencies
        run: |
          cd src/pipeline
          pip install -r requirements.txt

      - name: Azure Login
        uses: azure/login@v2
        with:
          client-id: ${{ secrets.AZURE_CLIENT_ID }}
          tenant-id: ${{ secrets.AZURE_TENANT_ID }}
          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}

      - name: Resolve endpoints for latest deployment
        run: |
          set -euo pipefail
          chmod +x ./scripts/get-endpoints.sh

          suffix="${{ secrets.AZURE_UNIQUE_SUFFIX }}"
          if [[ -n "$suffix" ]]; then
            out=$(./scripts/get-endpoints.sh "$AZURE_RESOURCE_GROUP" "$suffix")
          else
            out=$(./scripts/get-endpoints.sh "$AZURE_RESOURCE_GROUP")
          fi

          # Get endpoints from always-on infra (excludes Search which is ephemeral)
          echo "$out" | awk -F= '/^(OPENAI_ENDPOINT|SPEECH_ENDPOINT|SPEECH_REGION|DOCUMENT_INTELLIGENCE_ENDPOINT|COSMOS_DB_ENDPOINT|STORAGE_ACCOUNT_NAME)=/ {print $0}' >> "$GITHUB_ENV"
          
          # Use Search endpoint from ephemeral deployment
          echo "SEARCH_ENDPOINT=${{ needs.deploy-search.outputs.searchEndpoint }}" >> "$GITHUB_ENV"

      - name: Ensure Cosmos, Speech, OpenAI, and Storage RBAC for this workflow principal
        run: |
          set -euo pipefail

          accessToken=$(az account get-access-token \
            --resource https://management.azure.com \
            --query accessToken -o tsv)
          export ACCESS_TOKEN="$accessToken"

          principalObjectId=$(python3 - <<'PY'
          import os, json, base64
          token = os.environ.get('ACCESS_TOKEN', '')
          parts = token.split('.')
          if len(parts) < 2:
            print('')
            raise SystemExit(0)
          payload = parts[1]
          payload += '=' * (-len(payload) % 4)
          data = base64.urlsafe_b64decode(payload.encode('utf-8')).decode('utf-8')
          claims = json.loads(data)
          print(claims.get('oid') or claims.get('objectId') or '')
          PY
          )

          if [[ -z "$principalObjectId" ]]; then
            echo "::warning::Could not determine automation principal objectId; skipping RBAC checks."
            exit 0
          fi
          echo "Principal objectId: $principalObjectId"

          # --- Cosmos SQL RBAC ---
          cosmosName=$(echo "$COSMOS_DB_ENDPOINT" | sed -E 's#^https?://([^./]+)\..*$#\1#')
          if [[ -z "$cosmosName" ]]; then
            echo "::error::Could not parse Cosmos account name from COSMOS_DB_ENDPOINT=$COSMOS_DB_ENDPOINT"
            exit 1
          fi

          existing=$(az cosmosdb sql role assignment list \
            --account-name "$cosmosName" \
            --resource-group "$AZURE_RESOURCE_GROUP" \
            --query "[?principalId=='$principalObjectId' && scope=='/dbs/certaudio'] | length(@)" \
            -o tsv || echo "0")

          if [[ "$existing" == "0" ]]; then
            echo "Creating Cosmos SQL role assignment..."
            az cosmosdb sql role assignment create \
              --account-name "$cosmosName" \
              --resource-group "$AZURE_RESOURCE_GROUP" \
              --principal-id "$principalObjectId" \
              --scope "/dbs/certaudio" \
              --role-definition-id "00000000-0000-0000-0000-000000000002" \
              --only-show-errors
          else
            echo "Cosmos SQL RBAC already exists."
          fi

          # NOTE: Azure AI Search RBAC is handled in deploy-search job via Bicep

          # --- Azure OpenAI RBAC ---
          openAiName=$(echo "$OPENAI_ENDPOINT" | sed -E 's#^https?://([^./]+)\..*$#\1#')
          if [[ -z "$openAiName" ]]; then
            echo "::error::Could not parse OpenAI account name from OPENAI_ENDPOINT=$OPENAI_ENDPOINT"
            exit 1
          fi

          openAiId=$(az resource show \
            --resource-group "$AZURE_RESOURCE_GROUP" \
            --name "$openAiName" \
            --resource-type "Microsoft.CognitiveServices/accounts" \
            --query id -o tsv)

          if [[ -z "$openAiId" ]]; then
            echo "::error::Could not resolve OpenAI resource ID for $openAiName"
            exit 1
          fi

          existing=$(az role assignment list \
            --assignee "$principalObjectId" \
            --scope "$openAiId" \
            --query "[?roleDefinitionName=='Cognitive Services OpenAI User'] | length(@)" \
            -o tsv || echo "0")

          if [[ "$existing" == "0" ]]; then
            echo "Creating Cognitive Services OpenAI User role assignment..."
            az role assignment create \
              --assignee-object-id "$principalObjectId" \
              --assignee-principal-type ServicePrincipal \
              --role "Cognitive Services OpenAI User" \
              --scope "$openAiId" \
              --only-show-errors

            # Role assignments can take a short time to propagate.
            echo "Waiting 30s for RBAC propagation..."
            sleep 30
          else
            echo "OpenAI RBAC already exists."
          fi

          # --- Azure Storage Blob RBAC ---
          storageId=$(az resource show \
            --resource-group "$AZURE_RESOURCE_GROUP" \
            --name "$STORAGE_ACCOUNT_NAME" \
            --resource-type "Microsoft.Storage/storageAccounts" \
            --query id -o tsv)

          if [[ -z "$storageId" ]]; then
            echo "::error::Could not resolve Storage resource ID for $STORAGE_ACCOUNT_NAME"
            exit 1
          fi

          existing=$(az role assignment list \
            --assignee "$principalObjectId" \
            --scope "$storageId" \
            --query "[?roleDefinitionName=='Storage Blob Data Contributor'] | length(@)" \
            -o tsv || echo "0")

          if [[ "$existing" == "0" ]]; then
            echo "Creating Storage Blob Data Contributor role assignment..."
            az role assignment create \
              --assignee-object-id "$principalObjectId" \
              --assignee-principal-type ServicePrincipal \
              --role "Storage Blob Data Contributor" \
              --scope "$storageId" \
              --only-show-errors

            # Role assignments can take a short time to propagate.
            echo "Waiting 30s for RBAC propagation..."
            sleep 30
          else
            echo "Storage RBAC already exists."
          fi

          # --- Azure Speech RBAC ---
          speechName=$(echo "$SPEECH_ENDPOINT" | sed -E 's#^https?://([^./]+)\..*$#\1#')
          if [[ -z "$speechName" ]]; then
            echo "::error::Could not parse Speech account name from SPEECH_ENDPOINT=$SPEECH_ENDPOINT"
            exit 1
          fi

          speechId=$(az resource show \
            --resource-group "$AZURE_RESOURCE_GROUP" \
            --name "$speechName" \
            --resource-type "Microsoft.CognitiveServices/accounts" \
            --query id -o tsv)

          if [[ -z "$speechId" ]]; then
            echo "::error::Could not resolve Speech resource ID for $speechName"
            exit 1
          fi

          existing=$(az role assignment list \
            --assignee "$principalObjectId" \
            --scope "$speechId" \
            --query "[?roleDefinitionName=='Cognitive Services Speech User'] | length(@)" \
            -o tsv || echo "0")

          if [[ "$existing" == "0" ]]; then
            echo "Creating Cognitive Services Speech User role assignment..."
            az role assignment create \
              --assignee-object-id "$principalObjectId" \
              --assignee-principal-type ServicePrincipal \
              --role "Cognitive Services Speech User" \
              --scope "$speechId" \
              --only-show-errors

            echo "Waiting 30s for RBAC propagation..."
            sleep 30
          else
            echo "Speech RBAC already exists."
          fi

      - name: Index content in AI Search
        env:
          SOURCE_URLS: ${{ needs.discover-content.outputs.sourceUrls }}
          CERTIFICATION_ID: ${{ inputs.certificationId }}
        run: |
          cd src/pipeline
          python3 -m tools.index_content \
            --certification-id "$CERTIFICATION_ID" \
            --source-urls "$SOURCE_URLS"

  generate-episodes:
    name: Generate Episodes
    runs-on: ubuntu-latest
    needs: [discover-content, deploy-search, index-content]
    strategy:
      fail-fast: false  # Continue other batches even if one fails
      matrix:
        batch: ${{ fromJson(needs.discover-content.outputs.batchIndices) }}
      max-parallel: 3  # Reduce parallelism to avoid rate limits
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: src/pipeline/requirements.txt

      - name: Install dependencies
        run: |
          cd src/pipeline
          pip install -r requirements.txt

      - name: Azure Login
        uses: azure/login@v2
        with:
          client-id: ${{ secrets.AZURE_CLIENT_ID }}
          tenant-id: ${{ secrets.AZURE_TENANT_ID }}
          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}

      - name: Resolve endpoints for latest deployment
        run: |
          set -euo pipefail
          chmod +x ./scripts/get-endpoints.sh

          suffix="${{ secrets.AZURE_UNIQUE_SUFFIX }}"
          if [[ -n "$suffix" ]]; then
            out=$(./scripts/get-endpoints.sh "$AZURE_RESOURCE_GROUP" "$suffix")
          else
            out=$(./scripts/get-endpoints.sh "$AZURE_RESOURCE_GROUP")
          fi

          # Get endpoints from always-on infra (excludes Search which is ephemeral)
          echo "$out" | awk -F= '/^(OPENAI_ENDPOINT|SPEECH_ENDPOINT|SPEECH_REGION|DOCUMENT_INTELLIGENCE_ENDPOINT|COSMOS_DB_ENDPOINT|STORAGE_ACCOUNT_NAME)=/ {print $0}' >> "$GITHUB_ENV"
          
          # Use Search endpoint from ephemeral deployment
          echo "SEARCH_ENDPOINT=${{ needs.deploy-search.outputs.searchEndpoint }}" >> "$GITHUB_ENV"

      - name: Generate episode batch
        env:
          CERTIFICATION_ID: ${{ inputs.certificationId }}
          AUDIO_FORMAT: ${{ inputs.audioFormat }}
          INSTRUCTIONAL_VOICE: ${{ inputs.instructionalVoice }}
          PODCAST_HOST_VOICE: ${{ inputs.podcastHostVoice }}
          PODCAST_EXPERT_VOICE: ${{ inputs.podcastExpertVoice }}
          FORCE_REGENERATE: ${{ inputs.forceRegenerate }}
          SKILLS_OUTLINE: ${{ needs.discover-content.outputs.skillsOutline }}
          BATCH_INDEX: ${{ matrix.batch }}
        run: |
          cd src/pipeline
          
          # Build command with optional --force-regenerate flag
          REGEN_FLAG=""
          if [ "$FORCE_REGENERATE" = "true" ]; then
            REGEN_FLAG="--force-regenerate"
          fi
          
          python3 -m tools.generate_episodes \
            --certification-id "$CERTIFICATION_ID" \
            --audio-format "$AUDIO_FORMAT" \
            --instructional-voice "$INSTRUCTIONAL_VOICE" \
            --podcast-host-voice "$PODCAST_HOST_VOICE" \
            --podcast-expert-voice "$PODCAST_EXPERT_VOICE" \
            --skills-outline "$SKILLS_OUTLINE" \
            --batch-index "$BATCH_INDEX" \
            --batch-size "$EPISODE_BATCH_SIZE" \
            $REGEN_FLAG

  finalize:
    name: Finalize Content Generation
    runs-on: ubuntu-latest
    needs: [discover-content, generate-episodes]
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: src/pipeline/requirements.txt

      - name: Azure Login
        uses: azure/login@v2
        with:
          client-id: ${{ secrets.AZURE_CLIENT_ID }}
          tenant-id: ${{ secrets.AZURE_TENANT_ID }}
          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}

      - name: Resolve endpoints for latest deployment
        run: |
          set -euo pipefail
          chmod +x ./scripts/get-endpoints.sh

          suffix="${{ secrets.AZURE_UNIQUE_SUFFIX }}"
          if [[ -n "$suffix" ]]; then
            out=$(./scripts/get-endpoints.sh "$AZURE_RESOURCE_GROUP" "$suffix")
          else
            out=$(./scripts/get-endpoints.sh "$AZURE_RESOURCE_GROUP")
          fi

          echo "$out" | awk -F= '/^(OPENAI_ENDPOINT|SPEECH_ENDPOINT|SPEECH_REGION|DOCUMENT_INTELLIGENCE_ENDPOINT|SEARCH_ENDPOINT|COSMOS_DB_ENDPOINT|STORAGE_ACCOUNT_NAME)=/ {print $0}' >> "$GITHUB_ENV"

      - name: Ensure Cosmos and Storage RBAC for this workflow principal
        run: |
          set -euo pipefail

          accessToken=$(az account get-access-token \
            --resource https://management.azure.com \
            --query accessToken -o tsv)
          export ACCESS_TOKEN="$accessToken"

          principalObjectId=$(python3 - <<'PY'
          import os, json, base64
          token = os.environ.get('ACCESS_TOKEN', '')
          parts = token.split('.')
          if len(parts) < 2:
            print('')
            raise SystemExit(0)
          payload = parts[1]
          payload += '=' * (-len(payload) % 4)
          data = base64.urlsafe_b64decode(payload.encode('utf-8')).decode('utf-8')
          claims = json.loads(data)
          print(claims.get('oid') or claims.get('objectId') or '')
          PY
          )

          if [[ -z "$principalObjectId" ]]; then
            echo "::warning::Could not determine automation principal objectId; skipping Cosmos SQL RBAC check."
            exit 0
          fi

          cosmosName=$(echo "$COSMOS_DB_ENDPOINT" | sed -E 's#^https?://([^./]+)\..*$#\1#')
          if [[ -z "$cosmosName" ]]; then
            echo "::error::Could not parse Cosmos account name from COSMOS_DB_ENDPOINT=$COSMOS_DB_ENDPOINT"
            exit 1
          fi

          existing=$(az cosmosdb sql role assignment list \
            --account-name "$cosmosName" \
            --resource-group "$AZURE_RESOURCE_GROUP" \
            --query "[?principalId=='$principalObjectId' && scope=='/dbs/certaudio'] | length(@)" \
            -o tsv || echo "0")

          if [[ "$existing" == "0" ]]; then
            az cosmosdb sql role assignment create \
              --account-name "$cosmosName" \
              --resource-group "$AZURE_RESOURCE_GROUP" \
              --principal-id "$principalObjectId" \
              --scope "/dbs/certaudio" \
              --role-definition-id "00000000-0000-0000-0000-000000000002" \
              --only-show-errors
          fi

          # --- Azure Storage Blob RBAC ---
          storageId=$(az resource show \
            --resource-group "$AZURE_RESOURCE_GROUP" \
            --name "$STORAGE_ACCOUNT_NAME" \
            --resource-type "Microsoft.Storage/storageAccounts" \
            --query id -o tsv)

          if [[ -z "$storageId" ]]; then
            echo "::error::Could not resolve Storage resource ID for $STORAGE_ACCOUNT_NAME"
            exit 1
          fi

          existing=$(az role assignment list \
            --assignee "$principalObjectId" \
            --scope "$storageId" \
            --query "[?roleDefinitionName=='Storage Blob Data Contributor'] | length(@)" \
            -o tsv || echo "0")

          if [[ "$existing" == "0" ]]; then
            echo "Creating Storage Blob Data Contributor role assignment..."
            az role assignment create \
              --assignee-object-id "$principalObjectId" \
              --assignee-principal-type ServicePrincipal \
              --role "Storage Blob Data Contributor" \
              --scope "$storageId" \
              --only-show-errors

            echo "Waiting 30s for RBAC propagation..."
            sleep 30
          else
            echo "Storage RBAC already exists."
          fi

      - name: Generate episode index
        env:
          CERTIFICATION_ID: ${{ inputs.certificationId }}
          AUDIO_FORMAT: ${{ inputs.audioFormat }}
        run: |
          cd src/pipeline
          pip install -r requirements.txt
          python3 -m tools.generate_index \
            --certification-id "$CERTIFICATION_ID" \
            --audio-format "$AUDIO_FORMAT" \
            --min-episodes "${{ needs.discover-content.outputs.mainSkillCount }}"

      - name: Output generation summary
        run: |
          echo "## Content Generation Complete! :headphones:" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Setting | Value |" >> $GITHUB_STEP_SUMMARY
          echo "|---------|-------|" >> $GITHUB_STEP_SUMMARY
          echo "| Certification | ${{ inputs.certificationId }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Format | ${{ inputs.audioFormat }} |" >> $GITHUB_STEP_SUMMARY

  # =========================================================================
  # CLEANUP - Delete ephemeral AI Search to save ~$250/month
  # Always runs, even if generation failed, to avoid leaving resources behind
  # =========================================================================
  cleanup-search:
    name: Delete Ephemeral AI Search
    needs: [deploy-search, generate-episodes, finalize]
    if: always() && needs.deploy-search.outputs.searchName != ''
    runs-on: ubuntu-latest
    steps:
      - name: Azure Login
        uses: azure/login@v2
        with:
          client-id: ${{ secrets.AZURE_CLIENT_ID }}
          tenant-id: ${{ secrets.AZURE_TENANT_ID }}
          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}

      - name: Delete AI Search service
        uses: azure/cli@v2
        with:
          inlineScript: |
            set -euo pipefail
            
            searchName="${{ needs.deploy-search.outputs.searchName }}"
            
            echo "Deleting ephemeral AI Search: $searchName"
            
            # Check if it exists first
            exists=$(az search service show \
              --name "$searchName" \
              --resource-group "$AZURE_RESOURCE_GROUP" \
              --query name -o tsv 2>/dev/null || echo "")
            
            if [[ -n "$exists" ]]; then
              az search service delete \
                --name "$searchName" \
                --resource-group "$AZURE_RESOURCE_GROUP" \
                --yes
              
              echo "âœ… AI Search deleted: $searchName"
              echo "   Monthly savings: ~\$250"
            else
              echo "âš ï¸ AI Search not found (may have been deleted already): $searchName"
            fi
      
      - name: Summary
        run: |
          echo "## ðŸ§¹ Cleanup Complete" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Ephemeral AI Search service deleted to save ~\$250/month." >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "The following always-on resources remain:" >> $GITHUB_STEP_SUMMARY
          echo "- Storage Account (audio files)" >> $GITHUB_STEP_SUMMARY
          echo "- Cosmos DB (episode metadata)" >> $GITHUB_STEP_SUMMARY
          echo "- Azure Functions (API)" >> $GITHUB_STEP_SUMMARY
          echo "- Static Web App (website)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Estimated monthly cost: ~\$5-10" >> $GITHUB_STEP_SUMMARY
